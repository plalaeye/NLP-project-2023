{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + 'train.csv')\n",
    "val = pd.read_csv(data_path + 'val.csv')\n",
    "test = pd.read_csv(data_path + 'test.csv')\n",
    "\n",
    "train = train[train['label'] == 0]\n",
    "val = val[val['label'] == 0]\n",
    "test = test[test['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = pd.read_csv(data_path + 'sample_train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 368640 || all params: 582769920 || trainable%: 0.06325652497644353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC21\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"thanathorn/mt5-cpe-kmutt-thai-sentence-sum\"\n",
    "tokenizer_name_or_path = \"thanathorn/mt5-cpe-kmutt-thai-sentence-sum\"\n",
    "\n",
    "max_length = 128\n",
    "lr = 5e-3\n",
    "epoch_count =  15\n",
    "\n",
    "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Impolite2PoliteDatasetWithPaddingLeft(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_length=128, device='cuda', padding_left=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.padding_left = padding_left\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        if self.padding_left:\n",
    "            tokenized_x = self.tokenizer(self.X[idx], return_tensors='pt', truncation=True, max_length=self.max_length)\n",
    "            tokenized_y = self.tokenizer(self.y[idx], return_tensors='pt', truncation=True, max_length=self.max_length)\n",
    "            for i in range(len(tokenized_x['input_ids'])):\n",
    "                concat_input = torch.cat((torch.zeros(128), tokenized_x['input_ids'][i]))\n",
    "                input_ids.append(concat_input[-128:])\n",
    "                concat_attention_mask = torch.cat((torch.zeros(128), tokenized_x['attention_mask'][i]))\n",
    "                attention_mask.append(concat_attention_mask[-128:])\n",
    "                concat_labels = torch.cat((torch.zeros(128), tokenized_y['input_ids'][i]))\n",
    "                labels.append(concat_labels[-128:])\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            attention_mask = torch.stack(attention_mask)\n",
    "            labels = torch.stack(labels)\n",
    "        else:\n",
    "            tokenized_x = self.tokenizer(self.X[idx], return_tensors='pt', truncation=True, max_length=self.max_length, padding='max_length')\n",
    "            tokenized_y = self.tokenizer(self.y[idx], return_tensors='pt', truncation=True, max_length=self.max_length, padding='max_length')\n",
    "            input_ids = tokenized_x['input_ids']\n",
    "            attention_mask = tokenized_x['attention_mask']\n",
    "            labels = tokenized_y['input_ids']\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "        return {'input_ids': input_ids[0].to(device), 'attention_mask': attention_mask[0].to(device), 'labels': labels[0].to(device)}\n",
    "        # return {'text': f'formalize: {self.X[idx]}', 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "impolite2polite_dataset = Impolite2PoliteDatasetWithPaddingLeft(sample_train['text'].tolist(), sample_train['clean'].tolist(), tokenizer, max_length=max_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "impolite2polite_dataloader = DataLoader(impolite2polite_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   259,  47820, 219802, 198423,  28456,  36233,   7428,   4388,      1,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'labels': tensor([[   259,  89804,  28456,  22254,  41259, 186382,   7428,      1,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "            -100,   -100]], device='cuda:0')}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(impolite2polite_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_model = GenerationModel(text_generation_model, text_generation_tokenizer)\n",
    "# style_transfer_model = StyleTransferModel(len(tokenized_prefix[0]), 10, tokenized_prefix, generation_model.generate_text)\n",
    "# style_transfer_model = style_transfer_model.to(device)\n",
    "# style_transfer_model.prefix_model = style_transfer_model.prefix_model.to(device)\n",
    "\n",
    "# for param in style_transfer_model.prefix_model.parameters():\n",
    "#     param.requires_grad_()\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(impolite2polite_dataloader) * epoch_count),\n",
    ")\n",
    "# classification_criterion = nn.CrossEntropyLoss()\n",
    "# bert_score = load(\"bertscore\")\n",
    "# bert_score_model = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 3.456629946231842\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 3.3461527466773986\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 3.1647611433267593\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 3.2160088127851485\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 3.120749422311783\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 3.037451884150505\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 2.9584812819957733\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 2.8917870575189593\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 2.9222706896066666\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 2.8867884743213654\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 2.758167324066162\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train loss: 2.8021338403224947\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train loss: 2.748575673699379\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train loss: 2.706773039996624\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train loss: 2.7550100407004354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = epoch_count\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    print(f'Epoch {epoch}')\n",
    "    model.train()\n",
    "    for batch in tqdm(impolite2polite_dataloader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        inp = {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "        output = model(**inp)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        # check if gradients are non-zero\n",
    "        # for name, param in style_transfer_model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(name, param.grad)\n",
    "        #     else:\n",
    "        #         print(name, 'None')\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "    # style_transfer_model.eval()\n",
    "    # val_loss = []\n",
    "    # for batch in tqdm(val_dataloader):\n",
    "    #     encoding = text_generation_tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True, max_length=256).input_ids\n",
    "    #     encoding = encoding.to(device).float()\n",
    "    #     output = style_transfer_model(encoding)\n",
    "    #     loss = classification_loss(classification_model, classification_tokenizer, classification_criterion, output) + content_loss(bert_score, output, batch, bert_score_model)\n",
    "    #     val_loss.append(loss.item())\n",
    "    # # style_transfer_model.save_pretrained(f'./checkpoints/model/{epoch}')\n",
    "    # torch.save(style_transfer_model.state_dict(), f'./checkpoints/model/{epoch}.pt')\n",
    "    print(f'Epoch {epoch} train loss: {np.mean(train_loss)}')\n",
    "    train_losses.append(np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./checkpoints/model/summarize_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = './checkpoints/model/summarize_2'\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,    259,  98179,   4215, 212384, 167595,      1]],\n",
      "       device='cuda:0')\n",
      "['แมคกูปะ']\n",
      "tensor([[    0,   259, 23045, 23248,  4682,     1]], device='cuda:0')\n",
      "['ออกวันนี้']\n",
      "tensor([[     0, 129967,  39350,  67641,  52638,  14166,      1]],\n",
      "       device='cuda:0')\n",
      "['อยากกินข้าวบ้างครับ']\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "i = 4\n",
    "word = ['แอคกูปะ', 'ตังออกวันไหน', 'พาไปหน่อยสิ อยากกิน']\n",
    "for w in word:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(w, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, early_stopping=True, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "        print(outputs)\n",
    "        print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
